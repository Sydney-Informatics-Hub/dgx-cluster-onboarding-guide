[
  {
    "objectID": "notebooks/data_transfer.html",
    "href": "notebooks/data_transfer.html",
    "title": "How to transfer data to and from the Persistent volume claim",
    "section": "",
    "text": "In this section we will describe methods of transferring data between the Research Data Store (RDS) and Gadi. For now, while we await the implementation of Globus for fast and efficient transfer to and from your Persistent Volume Claim (PVC), we will describe an interactive method for transferring data using a JupyterLab environment in the Run:ai web interface. In the future we will include instructions for copying data using the Run:AI CLI at the command line.\nHere we assume you already have set up a project and have some Persistent Volume Claim (PVC) storage available.\n\n\nYou can easily transfer data between your Persistent Volume Claim (PVC) and RDS from inside the Run:ai web browser interface. We have set up an environment called interactive_dt for you to do this.\nTo run the interactive_dt environment from a template:\n\nLog into the Run:ai dashboard at gpu.sydney.edu.au and use Okta to login with your credentials via the “CONTINUE WITH SSO” sign in option.\nClick ‘workloads’ in the left panel and then the blue ‘new workload’ icon in the top left of the workloads screen and select ‘workspace’.\n\n\n\n\nNew Workload\n\n\n\nSelect your project from the projects available and select the interactive-data-transfer template and give your workspace a name before clicking with your mouse cursor on CONTINUE.\nIf you have selected the interactive-data-transfer template, you should now have pre-populated the required interactive-dt environment and the data-transfer compute resource fields on the following page. You can double check this now.\nExpand the Data sources box and select the PVC associated with your project from the list.\n\n\n\n\nData Sources\n\n\n\nWhen you are happy with everything, click CREATE WORKSPACE and your data transfer environment will be created. When this is provisioned click the CONNECT icon above the list of workloads.\n\n\n\n\nConnect\n\n\n\nYou will again be prompted for your Run:ai login and your newly created JupyterLab session will appear in a new tab in your browser. Select the ‘Terminal’ app there.\n\n\n\n\nTerminal\n\n\n\nIn the open terminal app you can now use the rsync command to copy data to/from your project space in RDS.\n\nTo copy data from RDS to the GPU cluster, you can type the following into the open terminal:\n\n\n\n\n\n\nNote\n\n\n\nBe sure to replace everything in brackets &lt; &gt; with values specific to the data you are trying to copy as follows:\n&lt;your_unikey&gt;: Your University of Sydney unikey, usually in the form abcd0123.\n&lt;rds_project&gt;: The name of the project you have on DashR with data stored in RDS.\n&lt;path_to_project_data&gt;: The location of your storage directory under your project on RDS. (e.g. my_project_data/my_project_data_for_dgx/)\n&lt;pvc_mount_point&gt;: The location your PVC has been mounted (this is established when your PVC is provisioned). (e.g. /scratch)\n&lt;path_to_pvc_data&gt;: The path of the data you have stored on the PVC. (e.g. my_dgx_data/workflow_output/)\n\n\nrsync -rlctP &lt;your_unikey&gt;@research-data-int.sydney.edu.au:/rds/PRJ-&lt;rds_project&gt;/&lt;path_to_project_data&gt; &lt;pvc_mount_point&gt;/&lt;path_to_pvc_data&gt;\nAfter you execute this command, you will be prompted for the password associated with your unikey to establish a connection to RDS.\nTo copy data from the GPU cluster to RDS, reverse the order of source and destination in the above command:\nrsync -rlctP &lt;pvc_mount_point&gt;/&lt;path_to_pvc_data&gt; &lt;your_unikey&gt;@research-data-int.sydney.edu.au:/rds/PRJ-&lt;rds_project&gt;/&lt;path_to_project_data&gt; \nThe above rsync will also perform integrity checking using a checksum, comparing the original and copied files to make sure they are identical.\nDuring file transfer, for larger files, you can close the browser and leave things running in the background. You can then reconnect to check its status by logging back into the web UI at gpu.sydney.edu.au.\n\n\n\n\n\n\nWarningFor early adopters\n\n\n\nUntil we have correct user IDs and group IDs for your unikey and project set up in the PVC and the running workflow, you will need to update the permissions of the mounted PVC as root to be able to read/write files from there. To do this:\n\nChange to root\n\nsudo su\n\nUse chmod to set read and write for all on your mounted PVC space\n\nchmod -R 666 &lt;PVC_mount_point&gt;\nYou must do these steps BEFORE running the rsync command above.",
    "crumbs": [
      "How-to Guides",
      "How to transfer data to/from the SIH GPU Cluster"
    ]
  },
  {
    "objectID": "notebooks/data_transfer.html#interactive-data-transfer-tofrom-rds-from-a-web-browser",
    "href": "notebooks/data_transfer.html#interactive-data-transfer-tofrom-rds-from-a-web-browser",
    "title": "How to transfer data to and from the Persistent volume claim",
    "section": "",
    "text": "You can easily transfer data between your Persistent Volume Claim (PVC) and RDS from inside the Run:ai web browser interface. We have set up an environment called interactive_dt for you to do this.\nTo run the interactive_dt environment from a template:\n\nLog into the Run:ai dashboard at gpu.sydney.edu.au and use Okta to login with your credentials via the “CONTINUE WITH SSO” sign in option.\nClick ‘workloads’ in the left panel and then the blue ‘new workload’ icon in the top left of the workloads screen and select ‘workspace’.\n\n\n\n\nNew Workload\n\n\n\nSelect your project from the projects available and select the interactive-data-transfer template and give your workspace a name before clicking with your mouse cursor on CONTINUE.\nIf you have selected the interactive-data-transfer template, you should now have pre-populated the required interactive-dt environment and the data-transfer compute resource fields on the following page. You can double check this now.\nExpand the Data sources box and select the PVC associated with your project from the list.\n\n\n\n\nData Sources\n\n\n\nWhen you are happy with everything, click CREATE WORKSPACE and your data transfer environment will be created. When this is provisioned click the CONNECT icon above the list of workloads.\n\n\n\n\nConnect\n\n\n\nYou will again be prompted for your Run:ai login and your newly created JupyterLab session will appear in a new tab in your browser. Select the ‘Terminal’ app there.\n\n\n\n\nTerminal\n\n\n\nIn the open terminal app you can now use the rsync command to copy data to/from your project space in RDS.\n\nTo copy data from RDS to the GPU cluster, you can type the following into the open terminal:\n\n\n\n\n\n\nNote\n\n\n\nBe sure to replace everything in brackets &lt; &gt; with values specific to the data you are trying to copy as follows:\n&lt;your_unikey&gt;: Your University of Sydney unikey, usually in the form abcd0123.\n&lt;rds_project&gt;: The name of the project you have on DashR with data stored in RDS.\n&lt;path_to_project_data&gt;: The location of your storage directory under your project on RDS. (e.g. my_project_data/my_project_data_for_dgx/)\n&lt;pvc_mount_point&gt;: The location your PVC has been mounted (this is established when your PVC is provisioned). (e.g. /scratch)\n&lt;path_to_pvc_data&gt;: The path of the data you have stored on the PVC. (e.g. my_dgx_data/workflow_output/)\n\n\nrsync -rlctP &lt;your_unikey&gt;@research-data-int.sydney.edu.au:/rds/PRJ-&lt;rds_project&gt;/&lt;path_to_project_data&gt; &lt;pvc_mount_point&gt;/&lt;path_to_pvc_data&gt;\nAfter you execute this command, you will be prompted for the password associated with your unikey to establish a connection to RDS.\nTo copy data from the GPU cluster to RDS, reverse the order of source and destination in the above command:\nrsync -rlctP &lt;pvc_mount_point&gt;/&lt;path_to_pvc_data&gt; &lt;your_unikey&gt;@research-data-int.sydney.edu.au:/rds/PRJ-&lt;rds_project&gt;/&lt;path_to_project_data&gt; \nThe above rsync will also perform integrity checking using a checksum, comparing the original and copied files to make sure they are identical.\nDuring file transfer, for larger files, you can close the browser and leave things running in the background. You can then reconnect to check its status by logging back into the web UI at gpu.sydney.edu.au.\n\n\n\n\n\n\nWarningFor early adopters\n\n\n\nUntil we have correct user IDs and group IDs for your unikey and project set up in the PVC and the running workflow, you will need to update the permissions of the mounted PVC as root to be able to read/write files from there. To do this:\n\nChange to root\n\nsudo su\n\nUse chmod to set read and write for all on your mounted PVC space\n\nchmod -R 666 &lt;PVC_mount_point&gt;\nYou must do these steps BEFORE running the rsync command above.",
    "crumbs": [
      "How-to Guides",
      "How to transfer data to/from the SIH GPU Cluster"
    ]
  },
  {
    "objectID": "notebooks/dashboards.html",
    "href": "notebooks/dashboards.html",
    "title": "How to Use the Dashboards",
    "section": "",
    "text": "This dashboard view provides holistic infrastructure information useful for both researchers and system administrators in managing and planning the resources.\n\n\nThis section presents high-level statistics of the GPU computing resources \n\n\n\nReal-time monitoring of the cluster status in terms of GPU and CPU utilisation. \n\n\n\nInspecting all queueing jobs. Possible reasons why jobs are queueing include:\n\nThe number of GPUs requested to be allocated to the job has exceeded the remaining GPUs in the project.\nThe job is waiting for other jobs to finish before it can be scheduled. \n\n\n\n\nDisplaying the number of idle GPUs currently allocated to running workloads. \n\n\n\nSummary of the list of running workloads. \n\n\n\n\nThis dashboard provides more detailed breakdowns of the SIH GPU running status. Key statistics that are reported at separate levels:\n\nCluster\nProject\nWorkloads\nNodes",
    "crumbs": [
      "How-to Guides",
      "How to Use the Dashboards"
    ]
  },
  {
    "objectID": "notebooks/dashboards.html#overview",
    "href": "notebooks/dashboards.html#overview",
    "title": "How to Use the Dashboards",
    "section": "",
    "text": "This dashboard view provides holistic infrastructure information useful for both researchers and system administrators in managing and planning the resources.\n\n\nThis section presents high-level statistics of the GPU computing resources \n\n\n\nReal-time monitoring of the cluster status in terms of GPU and CPU utilisation. \n\n\n\nInspecting all queueing jobs. Possible reasons why jobs are queueing include:\n\nThe number of GPUs requested to be allocated to the job has exceeded the remaining GPUs in the project.\nThe job is waiting for other jobs to finish before it can be scheduled. \n\n\n\n\nDisplaying the number of idle GPUs currently allocated to running workloads. \n\n\n\nSummary of the list of running workloads.",
    "crumbs": [
      "How-to Guides",
      "How to Use the Dashboards"
    ]
  },
  {
    "objectID": "notebooks/dashboards.html#analytics",
    "href": "notebooks/dashboards.html#analytics",
    "title": "How to Use the Dashboards",
    "section": "",
    "text": "This dashboard provides more detailed breakdowns of the SIH GPU running status. Key statistics that are reported at separate levels:\n\nCluster\nProject\nWorkloads\nNodes",
    "crumbs": [
      "How-to Guides",
      "How to Use the Dashboards"
    ]
  },
  {
    "objectID": "notebooks/login.html",
    "href": "notebooks/login.html",
    "title": "Login",
    "section": "",
    "text": "Login\n\nIf you are not on the Sydney university network (e.g. working from home), you will need to connect to the VPN to see the login page.\nGo to: https://gpu.sydney.edu.au/.\n\n\n\n\nRun:ai login page\n\n\n\nClick “🔑 CONTINUE WITH SSO”. This will prompt you to use Okta to authenticate your login.\nOnce the login is successful, you’ll be presented with the landing (“Workloads”) page.\n\n\n\n\nRun:ai Workloads page",
    "crumbs": [
      "Getting Started with Run:ai",
      "Login"
    ]
  },
  {
    "objectID": "notebooks/environments.html",
    "href": "notebooks/environments.html",
    "title": "How to Configure Environments",
    "section": "",
    "text": "How to Configure Environments\nIn Run:AI, an environment is a configuration that defines the software setup needed to run your AI workloads. An environment typically includes:\n\nBase Docker image (e.g., pytorch/pytorch, tensorflow/tensorflow:2.20.0-jupyter)\nTools (such as Jupyter, RStudio, etc.)\nCustom runtime settings to run scripts or setup commands (e.g., installing extra packages, configuring the base URL, etc.) The SIH GPU platform has provided several environments for users to get started with:\n\n\nSelect “Environments” on the left panel, then click on “NEW ENVIRONMENT” \nIn the new window, select the right scope in which the new environment should be made available \nProvide a descriptive name and a simple description to the environment \nInsert the URL of the docker image. In this example, we’re pulling an NVIDIA Rapids docker image from Docker Hub with specific rapids, CUDA, and python versions: rapidsai/notebooks:25.10a-cuda12.0-py3.12 \nSpecify the Workload architecture & type depending on the workload you are intending to run. Hovering over the question mark icon to see more explanation on each option \nSelect and configure the right tool used to interact with the container. For instance, the below example configures a Jupyter server to run on the 8888 container port: \nThe “Runtime settings” are also critical in correctly configuring the container when it’s up and running. You often can find such information on the container registry, github repo, or by reading through the Dockerfile. In this example, we\n\nSet the command as jupyter-lab\nenable the remote access of the Jupyter lab (--ServerApp.allow_remote_access=True),\nset up the notebook root directory (--notebook-dir=/home/rapids/notebooks),\nautomatically populate the base url (--NotebookApp.base_url=/${RUNAI_PROJECT}/${RUNAI_JOB_NAME}) which is especially important to avoid the conflicts between multiple workloads using Jupyter as the front end entry\ndisable the token authentication (--NotebookApp.token='')\nAn additional environment variable is defined to install extra dependencies (click “ENVIRONMENT VARIABLE” then enter Name as EXTRA_PIP_PACKAGES and Value as beautifulsoup4) \n\nUse the default UID and GID from the image \nFinally, select “CREATE ENVIRONMENT” to finish the setup.",
    "crumbs": [
      "How-to Guides",
      "How to Configure Environments"
    ]
  },
  {
    "objectID": "notebooks/dgx.html",
    "href": "notebooks/dgx.html",
    "title": "Introduction to the SIH GPU Cluster",
    "section": "",
    "text": "Important\n\n\n\nCOMING SOON! We expect the SIH GPU cluster to be available for researchers to use in October 2025. Stay tuned for updates from the Sydney Informatics Hub!\n\n\n\nIntroduction to the SIH GPU Cluster\nThe SIH GPU Cluster is a high-performance computing (HPC) platform designed to accelerate AI and machine learning workloads. It is built on NVIDIA’s SIH GPU systems, which are purpose-built for deep learning and AI research. The cluster provides researchers with access to powerful GPUs, high-speed networking, and a robust software stack to facilitate development in AI research and scientific simulations.\nMore information about the SIH GPU Cluster can be found on the Sydney Informatics Hub’s Research Computing page.\n\n\nExpressions of Interest\nThe SIH GPU Cluster is currently in its final stages of deployment and is expected to be available for researchers to use in October 2025. If you are interested in using the SIH GPU Cluster for your research projects, please express your interest by filling out the Expression of Interest Form. This will help us gauge demand and plan for user onboarding and support.",
    "crumbs": [
      "Getting Started with the SIH GPU Cluster",
      "Introduction to the SIH GPU Cluster"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "1. Get a client\nThese are the setup instructions…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SIH GPU Cluster Onboarding Guide",
    "section": "",
    "text": "Note\n\n\n\nThe SIH GPU Cluster is managed by the Sydney Informatics Hub at the University of Sydney. Contact sih.info@sydney.edu.au for more questions.\n\n\n\n\n\nSection\nContent\n\n\nGetting Started with the SIH GPU Cluster\nCluster fundamentals\n\n\nGetting Started with Run:ai\nRun:ai fundamentals\n\n\nHow-to Guides\nProvide step-by-step instructions",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "notebooks/jupyter_tutorial.html",
    "href": "notebooks/jupyter_tutorial.html",
    "title": "Tutorial: Running a Jupyter Lab Workload",
    "section": "",
    "text": "A workload is the actual job or task you want to run on the platform. This could be training an AI model, and running an inference model and exposing its endpoint, doing data preprocessing, or conducting a scientific simulation.\nGenerally, the minimum requirements you need before creating the workload include:\n\nBeing granted permission to an active project\nAn environment to run such job\nHave created a data source, e.g. a PVC, to store your input and output data\nUnderstand the compute resources you need to run the job and have the option available under Compute Resources\n\nIn this tutorial, we will create a simple Jupyter Lab workload that allows you to run Jupyter notebooks interactively on the SIH GPU cluster.\n\n\nNavigate to the Workloads section of the platform and click on the “NEW WORKLOAD” button. Select “Workspace” from the dropdown menu. \n\n\n\nFill in the necessary details for your workload:\n\nUnder “Projects” select the project it will be linked to\nUnder “Templates” select “Start from sratch” (do not use any existing template) \nUnder “Environment” select the Jupyter Lab container environment you want to run \nUnder “Compute resource” select the resources required. \n\nThere are other optional components you can add to a workload depending on the needs of your task. These include:\n\nVolume (i.e. temporary data storage)\nData Sources (e.g. PVCs)\nOther general settings \n\n\n\n\nBesides the project allocation, all the other workload components can be populated from a pre-defined template:",
    "crumbs": [
      "Tutorials",
      "Creating a Jupyter Lab Workload"
    ]
  },
  {
    "objectID": "notebooks/jupyter_tutorial.html#step-1-create-a-workload",
    "href": "notebooks/jupyter_tutorial.html#step-1-create-a-workload",
    "title": "Tutorial: Running a Jupyter Lab Workload",
    "section": "",
    "text": "Navigate to the Workloads section of the platform and click on the “NEW WORKLOAD” button. Select “Workspace” from the dropdown menu.",
    "crumbs": [
      "Tutorials",
      "Creating a Jupyter Lab Workload"
    ]
  },
  {
    "objectID": "notebooks/jupyter_tutorial.html#step-2-configure-the-workload-from-scratch",
    "href": "notebooks/jupyter_tutorial.html#step-2-configure-the-workload-from-scratch",
    "title": "Tutorial: Running a Jupyter Lab Workload",
    "section": "",
    "text": "Fill in the necessary details for your workload:\n\nUnder “Projects” select the project it will be linked to\nUnder “Templates” select “Start from sratch” (do not use any existing template) \nUnder “Environment” select the Jupyter Lab container environment you want to run \nUnder “Compute resource” select the resources required. \n\nThere are other optional components you can add to a workload depending on the needs of your task. These include:\n\nVolume (i.e. temporary data storage)\nData Sources (e.g. PVCs)\nOther general settings",
    "crumbs": [
      "Tutorials",
      "Creating a Jupyter Lab Workload"
    ]
  },
  {
    "objectID": "notebooks/jupyter_tutorial.html#optional-create-a-workload-from-a-template",
    "href": "notebooks/jupyter_tutorial.html#optional-create-a-workload-from-a-template",
    "title": "Tutorial: Running a Jupyter Lab Workload",
    "section": "",
    "text": "Besides the project allocation, all the other workload components can be populated from a pre-defined template:",
    "crumbs": [
      "Tutorials",
      "Creating a Jupyter Lab Workload"
    ]
  },
  {
    "objectID": "notebooks/projects.html",
    "href": "notebooks/projects.html",
    "title": "How to Manage Projects",
    "section": "",
    "text": "How to Manage Projects\n\n\n\n\n\n\nNote\n\n\n\nTHIS SECTION IS PENDING ON DUE TO ONGOING DEVELOPMENT RELATED TO USER AUTHENTICATION BY THE USYD ICT IDENTITY TEAM.\n\n\nOnce users are granted access to the SIH GPU cluster, projects are created in Run:ai to align with their DashR projects.",
    "crumbs": [
      "How-to Guides",
      "How to Manage Projects"
    ]
  },
  {
    "objectID": "notebooks/storage.html",
    "href": "notebooks/storage.html",
    "title": "How to manage the persistent volume claim",
    "section": "",
    "text": "How to manage the persistent volume claim"
  },
  {
    "objectID": "notebooks/CLI.html",
    "href": "notebooks/CLI.html",
    "title": "The Run:AI Command Line Interface (CLI)",
    "section": "",
    "text": "The Run:AI Command Line Interface (CLI) is a tool that allows researchers to manage and run workloads directly from the terminal. It provides commands to submit, monitor, and control jobs on the SIH GPU cluster, as well as to manage projects, resources, and configurations. Using the CLI, users can interact with Run:AI’s platform without needing to access the graphical interface.\n\n\nTo set up the CLI in your terminal:\n\nLog into the Run:AI web interface and select the ‘Researcher Command Line Interface’ from the drop-down menu under the ‘?’ icon in the top right.\n\n\n\nSelect your preferred operating system and copy the command indicated in the box using the icon on the right - then paste this command into a terminal session on your local machine.\n\n\n\nFollow the prompts in your terminal to set up the CLI. Once complete you should now be able to start the CLI in a terminal using runai login at the command line.\n\nOnce the Run:AI CLI is set up - you can start a workflow by running a saved docker image of your choice. SIH have provided base docker images with a pre-installed set of common dependencies for GPU (sydneyinformaticshub/dgx-interactive-gpu) and CPU (sydneyinformaticshub/dgx-interactive-cpu) workflows on dockerhub, including basic packages for interactive use (e.g. ipython).\n\n\n\nYou can start a workload from a terminal session on your own laptop as long as you are connected to the University VPN. You can run this interactively which provides a simple terminal environment running inside the SIH GPU cluster.\n\n\n\nLogin to the Run:AI CLI at the command line:\n\nrunai login\nyou will be prompted for your password and Okta credentials in a browser window during this step.\n\nSet your project (replace &lt;my project&gt; with the name of your project):\n\nrunai project set &lt;my_project&gt;\n\nTo run an sydneyinformaticshub/dgx-interactive-gpu container with an interactive terminal session including mounting your projects existing PVC in /scratch inside the container you can use the following command (be sure to replace everything in brackets &lt;...&gt; with values specific to your requirements):\n\nrunai workspace submit &lt;workspace-name&gt; --image sydneyinformaticshub/dgx-interactive-gpu --gpu-devices-request 1 --cpu-core-request 1.0 --run-as-user --existing-pvc claimname=&lt;pvc-name&gt;,path=/scratch --attach\nHere is a brief rundown of the arguments of the command above:\n\nrunai workspace submit &lt;workspace-name&gt; will run a new workspace and give it the name specified in &lt;workspace-name&gt;\n--image sydneyinformaticshub/dgx-interactive-gpu will run the base Docker image located at sydneyinformaticshub/dgx-interactive-gpu, you can replace this image with your own, perhaps built yourself with extra package installs and using this image as a base\n--gpu-devices-request 1 --cpu-core-request 1.0 requests 1 GPU and 1 CPU for the workflow. There are multiple options for selecting GPU and CPU RAM and devices, see here or use runai wrkspace submit --help for a full list of options\n--run-as-user will run the workflow using your user id and group ids inherited from DashR for your project. These will be the user and groups for the account you logged into Run:AI with in step 1 above. You should normally use this option otherwise user and group ids may not be set up correctly inside your workspace\n--existing-pvc claimname=&lt;pvc-name&gt;,path=/scratch will mount an existing PVC associated with your project into the running workload. Replace  with the name of your PVC. This will mount the PVC into /scratch inside the running container - you can change this mount point to whatever you prefer inside the running workload. You can also omit this flag entirely if you do not intend to use a PVC in your workspace.\n--attach will run the container and attach to it, which in this case will provide an interactive shell session inside it.\n\n\n\n\n\nUsing the CLI it is also possible to set up a workflow that persists in the background. You can then connect to it whenever you like or as many times as you like - this is a useful option if you want to reserve resources that you can keep available as you require or you want to share resources interactively amongst multiple users.\nTo set this up follow the steps 1 and 2 above, then change the command in step 3 to:\nrunai workspace submit &lt;workspace-name&gt; --image sydneyinformaticshub/dgx-interactive-gpu --gpu-devices-request 1 --cpu-core-request 1.0 --run-as-user --existing-pvc claimname=&lt;pvc-name&gt;,path=/scratch --command -- bash -c 'trap : TERM INT; sleep infinity & wait'\nThis will run the workload and keep it persisting in the background. You can then connect to this container whenever you like using:\nrunai workspace bash &lt;workspace-name&gt;\nyou can use this method to connect to any running workspace on the cluster as well as connect multiple terminals inside the running workspace.\nMake sure you terminate the background workspace when you are finished. You can do this using:\nrunai workspace suspend &lt;workspace-name&gt;\nto suspend the workspace so you can restart it again later or\nrunai workspace delete &lt;workspace-name&gt;\nto delete the workspace entirely. Please note that when deleting the workspace you will lose all data inside it not saved to a mounted PVC.",
    "crumbs": [
      "How-to Guides",
      "How to run a terminal using the Command Line Interface (CLI)"
    ]
  },
  {
    "objectID": "notebooks/CLI.html#setting-up-the-runai-cli",
    "href": "notebooks/CLI.html#setting-up-the-runai-cli",
    "title": "The Run:AI Command Line Interface (CLI)",
    "section": "",
    "text": "To set up the CLI in your terminal:\n\nLog into the Run:AI web interface and select the ‘Researcher Command Line Interface’ from the drop-down menu under the ‘?’ icon in the top right.\n\n\n\nSelect your preferred operating system and copy the command indicated in the box using the icon on the right - then paste this command into a terminal session on your local machine.\n\n\n\nFollow the prompts in your terminal to set up the CLI. Once complete you should now be able to start the CLI in a terminal using runai login at the command line.\n\nOnce the Run:AI CLI is set up - you can start a workflow by running a saved docker image of your choice. SIH have provided base docker images with a pre-installed set of common dependencies for GPU (sydneyinformaticshub/dgx-interactive-gpu) and CPU (sydneyinformaticshub/dgx-interactive-cpu) workflows on dockerhub, including basic packages for interactive use (e.g. ipython).",
    "crumbs": [
      "How-to Guides",
      "How to run a terminal using the Command Line Interface (CLI)"
    ]
  },
  {
    "objectID": "notebooks/CLI.html#how-to-run-a-terminal-environment-at-the-command-line",
    "href": "notebooks/CLI.html#how-to-run-a-terminal-environment-at-the-command-line",
    "title": "The Run:AI Command Line Interface (CLI)",
    "section": "",
    "text": "You can start a workload from a terminal session on your own laptop as long as you are connected to the University VPN. You can run this interactively which provides a simple terminal environment running inside the SIH GPU cluster.\n\n\n\nLogin to the Run:AI CLI at the command line:\n\nrunai login\nyou will be prompted for your password and Okta credentials in a browser window during this step.\n\nSet your project (replace &lt;my project&gt; with the name of your project):\n\nrunai project set &lt;my_project&gt;\n\nTo run an sydneyinformaticshub/dgx-interactive-gpu container with an interactive terminal session including mounting your projects existing PVC in /scratch inside the container you can use the following command (be sure to replace everything in brackets &lt;...&gt; with values specific to your requirements):\n\nrunai workspace submit &lt;workspace-name&gt; --image sydneyinformaticshub/dgx-interactive-gpu --gpu-devices-request 1 --cpu-core-request 1.0 --run-as-user --existing-pvc claimname=&lt;pvc-name&gt;,path=/scratch --attach\nHere is a brief rundown of the arguments of the command above:\n\nrunai workspace submit &lt;workspace-name&gt; will run a new workspace and give it the name specified in &lt;workspace-name&gt;\n--image sydneyinformaticshub/dgx-interactive-gpu will run the base Docker image located at sydneyinformaticshub/dgx-interactive-gpu, you can replace this image with your own, perhaps built yourself with extra package installs and using this image as a base\n--gpu-devices-request 1 --cpu-core-request 1.0 requests 1 GPU and 1 CPU for the workflow. There are multiple options for selecting GPU and CPU RAM and devices, see here or use runai wrkspace submit --help for a full list of options\n--run-as-user will run the workflow using your user id and group ids inherited from DashR for your project. These will be the user and groups for the account you logged into Run:AI with in step 1 above. You should normally use this option otherwise user and group ids may not be set up correctly inside your workspace\n--existing-pvc claimname=&lt;pvc-name&gt;,path=/scratch will mount an existing PVC associated with your project into the running workload. Replace  with the name of your PVC. This will mount the PVC into /scratch inside the running container - you can change this mount point to whatever you prefer inside the running workload. You can also omit this flag entirely if you do not intend to use a PVC in your workspace.\n--attach will run the container and attach to it, which in this case will provide an interactive shell session inside it.",
    "crumbs": [
      "How-to Guides",
      "How to run a terminal using the Command Line Interface (CLI)"
    ]
  },
  {
    "objectID": "notebooks/CLI.html#run-a-workflow-in-the-background-and-connect-to-it-using-the-cli",
    "href": "notebooks/CLI.html#run-a-workflow-in-the-background-and-connect-to-it-using-the-cli",
    "title": "The Run:AI Command Line Interface (CLI)",
    "section": "",
    "text": "Using the CLI it is also possible to set up a workflow that persists in the background. You can then connect to it whenever you like or as many times as you like - this is a useful option if you want to reserve resources that you can keep available as you require or you want to share resources interactively amongst multiple users.\nTo set this up follow the steps 1 and 2 above, then change the command in step 3 to:\nrunai workspace submit &lt;workspace-name&gt; --image sydneyinformaticshub/dgx-interactive-gpu --gpu-devices-request 1 --cpu-core-request 1.0 --run-as-user --existing-pvc claimname=&lt;pvc-name&gt;,path=/scratch --command -- bash -c 'trap : TERM INT; sleep infinity & wait'\nThis will run the workload and keep it persisting in the background. You can then connect to this container whenever you like using:\nrunai workspace bash &lt;workspace-name&gt;\nyou can use this method to connect to any running workspace on the cluster as well as connect multiple terminals inside the running workspace.\nMake sure you terminate the background workspace when you are finished. You can do this using:\nrunai workspace suspend &lt;workspace-name&gt;\nto suspend the workspace so you can restart it again later or\nrunai workspace delete &lt;workspace-name&gt;\nto delete the workspace entirely. Please note that when deleting the workspace you will lose all data inside it not saved to a mounted PVC.",
    "crumbs": [
      "How-to Guides",
      "How to run a terminal using the Command Line Interface (CLI)"
    ]
  },
  {
    "objectID": "notebooks/user_interface.html",
    "href": "notebooks/user_interface.html",
    "title": "Navigating the User Interface",
    "section": "",
    "text": "Navigating the User Interface\n On the left panel, there are several options to select:\n\nDashboards: Two system dashboards, namely “Overview” and “Analytics”, are accessible to users. They provide both system- and project-level information including system summaries, real-time resource allocation, cluster load, etc.\nProjects: This lists out the projects the user has been assigned to.\nWorkloads: This page provides a summary of the current workloads and allows users to create and configure new workloads.\nEnvironments: Both platform-wide and customised environments can be found in this page.\nData Sources: This page allows users to configure new data sources and view existing ones.\nCompute Resources: This page summaries all compute resources (similar to choosing the “flavours” in a cloud computing environment) and allows users to create new compute resources for their specific needs.\nTemplates: This feature allows users to manage bespoke templates configured for their specific workloads.\nCredentials: This space allows users to define secrets including access keys, passwords, or other sensitive information essential to the execution of workloads during runtime.",
    "crumbs": [
      "Getting Started with Run:ai",
      "Navigating the User Interface"
    ]
  }
]